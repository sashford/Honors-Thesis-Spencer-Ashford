{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOo+QyUeklmELZ4Q6HzwoWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashford/Honors-Thesis-Spencer-Ashford/blob/main/TorchSparseDenoiser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "i9Q_wjsOBF9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7WJ8P8Rr926",
        "outputId": "7a6e53a4-a233-42d9-8eb8-8523700920e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "libavcodec-extra is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "libopenblas-dev is already the newest version (0.3.8+ds-1ubuntu0.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n",
            "python3 is already the newest version (3.8.2-0ubuntu2).\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "python-dev-is-python2 is already the newest version (2.7.17-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "subversion is already the newest version (1.13.0-3ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Checked out revision 8.\n",
            "Requirement already satisfied: Cmake in /usr/local/lib/python3.10/dist-packages (3.25.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install ffmpeg libavcodec-extra\n",
        "!sudo apt-get install libopenblas-dev build-essential\n",
        "!sudo apt-get install python3 python-dev python3-dev\n",
        "!apt install subversion\n",
        "!svn checkout https://github.com/sashford/Honors-Thesis-Spencer-Ashford.git\n",
        "!pip3 install Cmake\n",
        "!pip3 install ninja\n",
        "!pip install git+https://github.com/NVIDIA/MinkowskiEngine.git -v"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1VIUnXDvFtj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split, Subset, Dataset\n",
        "from torch import nn\n",
        "from torch.nn.modules.loss import _Loss\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn import linear_model\n",
        "\n",
        "import MinkowskiEngine as ME\n",
        "\n",
        "import librosa\n",
        "from IPython.display import display, Audio"
      ],
      "metadata": {
        "id": "WexAU3tisNlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotter helper"
      ],
      "metadata": {
        "id": "3ANnI5zl_15R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "azi = 120\n",
        "binsA = 90\n",
        "minR = 1\n",
        "maxR = 30\n",
        "binsR = 512\n",
        "\n",
        "def plotter(raw_input, target_input, processed_input=None):\n",
        "\n",
        "  if processed_input != None:\n",
        "    num_inputs = 3\n",
        "    fig, (ax1, ax3, ax2) = plt.subplots(1,num_inputs, subplot_kw=dict(projection='polar'))\n",
        "  else:\n",
        "    num_inputs = 2\n",
        "    fig, (ax1, ax3) = plt.subplots(1,num_inputs, subplot_kw=dict(projection='polar'))\n",
        "\n",
        "  ax1.set_title(\"Noisy\")\n",
        "  ax1.set_theta_zero_location(\"N\")\n",
        "  ax1.set_thetamin(-azi/2)\n",
        "  ax1.set_thetamax(azi/2)\n",
        "  ax1.grid(False)\n",
        "\n",
        "  if num_inputs == 3:\n",
        "    ax2.set_title(\"Denoised\")\n",
        "    ax2.set_theta_zero_location(\"N\")\n",
        "    ax2.set_thetamin(-azi/2)\n",
        "    ax2.set_thetamax(azi/2)\n",
        "    ax2.grid(False)\n",
        "\n",
        "  ax3.set_title(\"Noiseless\")\n",
        "  ax3.set_theta_zero_location(\"N\")\n",
        "  ax3.set_thetamin(-azi/2)\n",
        "  ax3.set_thetamax(azi/2)\n",
        "  ax3.grid(False)\n",
        "\n",
        "  theta = np.linspace(-azi/2, azi/2, binsA) * np.pi / 180\n",
        "  r = np.linspace(minR, maxR, binsR)\n",
        "  T, R = np.meshgrid(theta,r)\n",
        "  z = np.zeros_like(T)\n",
        "\n",
        "  plot_noise = ax1.pcolormesh(T, R, z, cmap='gray', shading='auto', vmin=0, vmax=1)\n",
        "  if num_inputs == 3:\n",
        "    plot_denoised = ax2.pcolormesh(T, R, z, cmap='gray', shading='auto', vmin=0, vmax=1)\n",
        "  plot_noiseless = ax3.pcolormesh(T, R, z, cmap='gray', shading='auto', vmin=0, vmax=1)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  if isinstance(raw_input, tuple):\n",
        "    raw_input = raw_input[0]\n",
        "  if isinstance(target_input, tuple):\n",
        "    target_input = target_input[0]\n",
        "  if num_inputs == 3 and isinstance(processed_input, tuple):\n",
        "    processed_input = processed_input[0]\n",
        "\n",
        "  plot_noise.set_array(raw_input.numpy().ravel())\n",
        "  if num_inputs == 3:\n",
        "    plot_denoised.set_array(processed_input.numpy().ravel())\n",
        "  plot_noiseless.set_array(target_input.numpy().ravel())\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FELkbsBrkHye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Definitions"
      ],
      "metadata": {
        "id": "_ml2XRODAFRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "def npy_loader(path):\n",
        "    sonar_data = torch.from_numpy(np.load(path))\n",
        "    return sonar_data\n",
        "\n",
        "def generate_sparse_tensor(input):\n",
        "  input_sparse = input.to_sparse(layout=torch.sparse_coo)\n",
        "  input_values = input_sparse.values()\n",
        "  input_indices = input_sparse.indices()\n",
        "\n",
        "torch.set_printoptions(precision=5)\n",
        "\n",
        "class Indices(deque):\n",
        "  def __init__(self):\n",
        "    super().__init__(self)\n",
        "    for i in range(batch_size):\n",
        "      self.append(None)\n",
        "\n",
        "  def next_index(self, index):\n",
        "    self.append(index)\n",
        "    self.popleft()\n",
        "\n",
        "\n",
        "class SonarDataset(Dataset):\n",
        "  def __init__(self):\n",
        "\n",
        "    root_folder = \"/content/Honors-Thesis-Spencer-Ashford.git/trunk/dataset/\"\n",
        "    self.noise_folder = datasets.DatasetFolder(root=root_folder + 'noise', loader=npy_loader, extensions=['.npy'])\n",
        "    self.noiseless_folder = datasets.DatasetFolder(root=root_folder + 'noiseless',loader=npy_loader, extensions=['.npy'])\n",
        "    self.index = None\n",
        "    self.batch_indices = Indices()\n",
        "\n",
        "  def get_noiseless(self, index):\n",
        "    return self.noiseless_folder[index][0]\n",
        "\n",
        "  def get_noisy(self, index):\n",
        "    return self.noise_folder[index][0]\n",
        "\n",
        "  def batch_targets(self):\n",
        "    batch_target = []\n",
        "    for index in self.batch_indices:\n",
        "      target = self.get_noiseless(index)\n",
        "\n",
        "      batch_target.append(target.unsqueeze(0))\n",
        "    return torch.cat(batch_target,0).unsqueeze(1)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    self.batch_indices.next_index(index)\n",
        "    noisy = self.get_noisy(index)\n",
        "\n",
        "    if noisy.is_cuda:\n",
        "      return noisy\n",
        "    else:\n",
        "      return noisy.cuda()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.noise_folder)"
      ],
      "metadata": {
        "id": "go18wWzssQPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder_conv = nn.Sequential(\n",
        "        nn.Flatten(0),\n",
        "        nn.Linear(in_features= binsA * binsR * batch_size, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=256, out_features=128),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.decoder_conv = nn.Sequential(\n",
        "        nn.Linear(in_features=128, out_features=256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=256, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features= binsA * binsR * batch_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Unflatten(dim=0, unflattened_size=[batch_size, binsR, binsA])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    encoded = self.encoder_conv(x)\n",
        "    return self.decoder_conv(encoded)"
      ],
      "metadata": {
        "id": "BQuPEoLPgzoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_loss(autoencoder, images):\n",
        "  loss = 0\n",
        "  values = images\n",
        "  for i in range(len(model_children)):\n",
        "    values = F.relu((model_children[i](values)))\n",
        "    loss += torch.mean(torch.abs(values))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "MaMUkvP4OxKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Initialization and Parameters"
      ],
      "metadata": {
        "id": "s0hQT5dkALUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "lr = 0.001\n",
        "reg_param = 0.001\n",
        "sonar_data = SonarDataset()\n",
        "\n",
        "train_data, val_data = random_split(sonar_data, [0.85, 0.15])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "\n",
        "model = AutoEncoder()\n",
        "model.cuda()\n",
        "\n",
        "model_children = list(model.children())\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "CaIzkLjl6gDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training/Validation"
      ],
      "metadata": {
        "id": "5UzkS7n3ATCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "\n",
        "  loader = iter(dataloader)\n",
        "  for i, input in enumerate(loader):\n",
        "    if input.size()[0] == batch_size:\n",
        "      optimizer.zero_grad()\n",
        "      print(input.size())\n",
        "      decoded_data = model(input)\n",
        "      noiseless_image = sonar_data.batch_targets().cuda()\n",
        "\n",
        "      loss = loss_fn(decoded_data, noiseless_image) + reg_param * sparse_loss(model, noiseless_image)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print(f'\\t partial train loss {i} (single batch): {loss.data}')#| sizes: {decoded_data.size()}. {np.shape(noiseless_image)}')\n",
        "      train_loss.append(loss.detach().cpu().numpy())\n",
        "      input.cpu()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return np.mean(train_loss)"
      ],
      "metadata": {
        "id": "UhFa5yKR7953"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(model, dataloader, loss_fn):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    conc_out = []\n",
        "    conc_label = []\n",
        "    loader = iter(dataloader)\n",
        "    for i, input in enumerate(loader):\n",
        "      if input.size()[0] == batch_size:\n",
        "        print(input.size())\n",
        "        decoded_data = model(input)\n",
        "        noiseless_images = sonar_data.batch_targets().cuda()\n",
        "\n",
        "        conc_out.append(decoded_data)\n",
        "        conc_label.append(noiseless_images)\n",
        "\n",
        "        output = decoded_data\n",
        "        label = noiseless_images\n",
        "        # print(f\"out: {output.size()} label: {label.size()}\")\n",
        "        input.cpu()\n",
        "\n",
        "    conc_out = torch.cat(conc_out)\n",
        "    conc_label = torch.cat(conc_label)\n",
        "    # print(conc_out.size())\n",
        "    # print(conc_label.size())\n",
        "\n",
        "    val_loss = loss_fn(conc_out, conc_label)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return val_loss.data"
      ],
      "metadata": {
        "id": "JLKsTbR89NFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_tensor(tensor):\n",
        "  comb_tensor = []\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    comb_tensor.append(tensor)\n",
        "\n",
        "  comb_tensor = torch.cat(comb_tensor, 0)\n",
        "  return comb_tensor\n",
        "\n",
        "def convert_to_dense(stensor):\n",
        "\n",
        "\n",
        "def plot_outputs(model):\n",
        "  index = np.random.randint(0,len(val_data))\n",
        "  input = val_data[index]\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    decoded_data = model(input)\n",
        "\n",
        "    denoised = decoded_data[0]\n",
        "\n",
        "    plotter(sonar_data.get_noisy(index), sonar_data.get_noiseless(index), denoised.cpu())"
      ],
      "metadata": {
        "id": "CSW-LUjkAIaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Loop"
      ],
      "metadata": {
        "id": "2dbe4K8fAY7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "history_da={'train_loss':[], 'val_loss':[]}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f'EPOCH {epoch + 1}/{num_epochs}')\n",
        "  train_loss = train_epoch(\n",
        "      model=model,\n",
        "      dataloader=train_loader,\n",
        "      loss_fn=loss_fn,\n",
        "      optimizer=optimizer\n",
        "  )\n",
        "  print(\"beginning validation\")\n",
        "  val_loss = test_epoch(\n",
        "      model=model,\n",
        "      dataloader=val_loader,\n",
        "      loss_fn=loss_fn\n",
        "  )\n",
        "  print(\"begin plotting\")\n",
        "  history_da['train_loss'].append(train_loss)\n",
        "  history_da['val_loss'].append(val_loss)\n",
        "  print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "  plot_outputs(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NdEkkZTV-gAu",
        "outputId": "e0d2a558-fd53-48a7-9f4e-9dacede25088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1/50\n",
            "torch.Size([4, 512, 90])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4, 1, 512, 90])) that is different to the input size (torch.Size([4, 512, 90])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t partial train loss 0 (single batch): 0.0007565435371361673\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 1 (single batch): 0.0005774709861725569\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 2 (single batch): 0.0005520402337424457\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 3 (single batch): 0.0004997157375328243\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 4 (single batch): 0.0002901458356063813\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 5 (single batch): 0.0006618062616325915\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 6 (single batch): 0.0003911512903869152\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 7 (single batch): 0.00031051994301378727\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 8 (single batch): 0.0005069591570645571\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 9 (single batch): 0.0003611561260186136\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 10 (single batch): 0.00028238308732397854\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 11 (single batch): 0.0002767632540781051\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 12 (single batch): 8.282416820293292e-05\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 13 (single batch): 8.11557038105093e-05\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 14 (single batch): 0.0003870422369800508\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 15 (single batch): 0.0003645939868874848\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 16 (single batch): 0.0003710757300723344\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 17 (single batch): 0.0004403855127748102\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 18 (single batch): 0.0002575570542830974\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 19 (single batch): 0.0003904896439053118\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 20 (single batch): 0.00022832880495116115\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 21 (single batch): 0.0004388672241475433\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 22 (single batch): 0.000476247223559767\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 23 (single batch): 0.0002205997152486816\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 24 (single batch): 0.00027749131550081074\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 25 (single batch): 0.00027609511744230986\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 26 (single batch): 0.00024754280457273126\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 27 (single batch): 0.000365734682418406\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 28 (single batch): 0.0003568445972632617\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 29 (single batch): 0.00010916371684288606\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 30 (single batch): 0.00020517544180620462\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 31 (single batch): 0.0004322361492086202\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 32 (single batch): 0.0004016371676698327\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 33 (single batch): 0.00041050472646020353\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 34 (single batch): 0.00035297469003126025\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 35 (single batch): 0.0003471012460067868\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 36 (single batch): 0.0004250133642926812\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 37 (single batch): 0.00029916406492702663\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 38 (single batch): 0.00037582762888632715\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 39 (single batch): 0.00027526725898496807\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 40 (single batch): 0.00028211864992044866\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 41 (single batch): 0.0005113605875521898\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 42 (single batch): 0.0001251941139344126\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 43 (single batch): 0.000368454959243536\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 44 (single batch): 0.00023532559862360358\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 45 (single batch): 0.00023363139189314097\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 46 (single batch): 0.0003352737403474748\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 47 (single batch): 0.00043660515802912414\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 48 (single batch): 0.0004060876090079546\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 49 (single batch): 0.00040703703416511416\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 50 (single batch): 0.00018976119463331997\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 51 (single batch): 0.0001505536347394809\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 52 (single batch): 0.0004499249334912747\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 53 (single batch): 0.0003279474040027708\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 54 (single batch): 0.00036288751289248466\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 55 (single batch): 0.0002355524047743529\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 56 (single batch): 0.0003335028886795044\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 57 (single batch): 0.00035973420017398894\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 58 (single batch): 0.00045653071720153093\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 59 (single batch): 0.0002904680441133678\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 60 (single batch): 0.0002549376804381609\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 61 (single batch): 0.0002775757748167962\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 62 (single batch): 0.00023536627122666687\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 63 (single batch): 0.00045082351425662637\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 64 (single batch): 0.00020490509632509202\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 65 (single batch): 0.0002960227138828486\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 66 (single batch): 0.00023156212409958243\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 67 (single batch): 0.00041838394827209413\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 68 (single batch): 0.000383909500669688\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 69 (single batch): 0.0003206038963980973\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 70 (single batch): 0.0004748192732222378\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 71 (single batch): 0.00034483164199627936\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 72 (single batch): 0.0003534628776833415\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 73 (single batch): 0.0004090227303095162\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 74 (single batch): 0.0004226284218020737\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 75 (single batch): 0.0003882227174472064\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 76 (single batch): 0.0004969388246536255\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 77 (single batch): 0.00019935611635446548\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 78 (single batch): 0.00034740802948363125\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 79 (single batch): 0.0004653500800486654\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 80 (single batch): 0.00013486339594237506\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 81 (single batch): 0.00027510421932674944\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 82 (single batch): 0.00045735854655504227\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 83 (single batch): 0.00039960394497029483\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 84 (single batch): 0.00035239465069025755\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 85 (single batch): 0.0004877879109699279\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 86 (single batch): 0.000428224477218464\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 87 (single batch): 0.00027338205836713314\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 88 (single batch): 0.0004823052731808275\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 89 (single batch): 0.000498858978971839\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 90 (single batch): 0.0002640365855768323\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 91 (single batch): 0.0003409207856748253\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 92 (single batch): 0.0002718229661695659\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 93 (single batch): 0.000252265454037115\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 94 (single batch): 0.00044872539001517\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 95 (single batch): 0.0003292513720225543\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 96 (single batch): 0.0002380430232733488\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 97 (single batch): 0.00023567424796056002\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 98 (single batch): 0.00029105524299666286\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 99 (single batch): 0.00018923553579952568\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 100 (single batch): 0.00028279118123464286\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 101 (single batch): 0.00038083308027125895\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 102 (single batch): 0.00045668554957956076\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 103 (single batch): 0.0002839938679244369\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 104 (single batch): 0.00018767049186863005\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 105 (single batch): 0.0004472221480682492\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 106 (single batch): 0.00030276511097326875\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 107 (single batch): 0.0003739412932191044\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 108 (single batch): 0.0003726582508534193\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 109 (single batch): 0.00035215113894082606\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 110 (single batch): 0.0002314211305929348\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 111 (single batch): 0.0001876385067589581\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 112 (single batch): 0.0003072221006732434\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 113 (single batch): 0.00042591450619511306\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 114 (single batch): 0.0004465230740606785\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 115 (single batch): 0.00026461045490577817\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 116 (single batch): 0.0003932242398150265\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 117 (single batch): 0.00011276905570412055\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 118 (single batch): 0.00033906870521605015\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 119 (single batch): 0.0002569208445493132\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 120 (single batch): 0.0003121241752523929\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 121 (single batch): 0.00027289154240861535\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 122 (single batch): 0.00030308656278066337\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 123 (single batch): 0.0005081328563392162\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 124 (single batch): 0.00017283050692640245\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 125 (single batch): 0.000196524677448906\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 126 (single batch): 0.0004062349325977266\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 127 (single batch): 0.00023270005476661026\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 128 (single batch): 0.00037500233156606555\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 129 (single batch): 0.0004063901142217219\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 130 (single batch): 0.00040253959014080465\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 131 (single batch): 0.00026291608810424805\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 132 (single batch): 0.00038582770503126085\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 133 (single batch): 0.00029273959808051586\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 134 (single batch): 0.0003998568281531334\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 135 (single batch): 0.00038829841651022434\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 136 (single batch): 0.0003384357551112771\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 137 (single batch): 0.00019876686565112323\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 138 (single batch): 0.00019731272186618298\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 139 (single batch): 0.00033828060259111226\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 140 (single batch): 0.0003325057914480567\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 141 (single batch): 0.00040846411138772964\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 142 (single batch): 0.0002668418746907264\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 143 (single batch): 0.00028628474683500826\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 144 (single batch): 0.000325123401125893\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 145 (single batch): 0.00026459191576577723\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 146 (single batch): 0.0004120351222809404\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 147 (single batch): 0.00042045803274959326\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 148 (single batch): 0.0003464990295469761\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 149 (single batch): 0.000381011632271111\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 150 (single batch): 0.0003524352505337447\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 151 (single batch): 0.00035559595562517643\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 152 (single batch): 0.0001908993726829067\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 153 (single batch): 0.000428876985097304\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 154 (single batch): 0.00027157284785062075\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 155 (single batch): 0.00031220013624988496\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 156 (single batch): 0.0002423088881187141\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 157 (single batch): 0.00043664168333634734\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 158 (single batch): 0.00026279318262822926\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 159 (single batch): 0.00042707438115030527\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 160 (single batch): 0.00026467806310392916\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 161 (single batch): 0.0004079801728948951\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 162 (single batch): 0.0003720240783877671\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 163 (single batch): 0.0002784203097689897\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 164 (single batch): 0.0002740748750511557\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 165 (single batch): 0.00028913895948790014\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 166 (single batch): 0.00038093840703368187\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 167 (single batch): 0.00027359448722563684\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 168 (single batch): 0.0003681554808281362\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 169 (single batch): 0.0002745490928646177\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 170 (single batch): 0.0003196202451363206\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 171 (single batch): 0.0001722198212519288\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 172 (single batch): 0.00035250329528935254\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 173 (single batch): 0.00011915650247829035\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 174 (single batch): 0.00036759336944669485\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 175 (single batch): 0.00027539191069081426\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 176 (single batch): 0.00032030793954618275\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 177 (single batch): 0.0002855878265108913\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 178 (single batch): 0.0002801655500661582\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 179 (single batch): 0.00026114523643627763\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 180 (single batch): 0.00020122066780459136\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 181 (single batch): 0.0004335656703915447\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 182 (single batch): 0.00035390161792747676\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 183 (single batch): 0.00023321017215494066\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 184 (single batch): 0.0001939143257914111\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 185 (single batch): 0.0003863878082484007\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 186 (single batch): 0.0004612478951457888\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 187 (single batch): 0.00027914714883081615\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 188 (single batch): 0.000285166286630556\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 189 (single batch): 0.0003100016911048442\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 190 (single batch): 0.0002667674270924181\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 191 (single batch): 0.0002468778984621167\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 192 (single batch): 0.00025333979283459485\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 193 (single batch): 0.0003405461320653558\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 194 (single batch): 0.00029605775489471853\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 195 (single batch): 0.00021145770733710378\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 196 (single batch): 0.00034327610046602786\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 197 (single batch): 0.0003001983859576285\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 198 (single batch): 0.00031317307730205357\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 199 (single batch): 0.0003337708185426891\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 200 (single batch): 0.00031978890183381736\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 201 (single batch): 0.00016683693684171885\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 202 (single batch): 0.0002637432480696589\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 203 (single batch): 0.0003237886412534863\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 204 (single batch): 5.160821456229314e-05\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 205 (single batch): 0.0003502217005006969\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 206 (single batch): 0.00029870422440581024\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 207 (single batch): 0.0003338758833706379\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 208 (single batch): 0.00018230293062515557\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 209 (single batch): 0.00029273153631947935\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 210 (single batch): 0.00010342638415750116\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 211 (single batch): 0.00027111751842312515\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 212 (single batch): 0.0005216330173425376\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 213 (single batch): 0.00034229998709633946\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 214 (single batch): 0.00018783548148348927\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 215 (single batch): 0.0003557268646545708\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 216 (single batch): 0.0003064257907681167\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 217 (single batch): 0.000383533479180187\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 218 (single batch): 0.00029829624691046774\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 219 (single batch): 0.00042075119563378394\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 220 (single batch): 0.00021581583132501692\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 221 (single batch): 0.00031276626395992935\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 222 (single batch): 0.0003455566766206175\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 223 (single batch): 0.00034550807322375476\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 224 (single batch): 0.0002657785953488201\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 225 (single batch): 0.00034751277416944504\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 226 (single batch): 0.00031076709274202585\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 227 (single batch): 0.00029473984614014626\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 228 (single batch): 0.000480753107694909\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 229 (single batch): 0.00041595278889872134\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 230 (single batch): 0.00042985010077245533\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 231 (single batch): 0.0001958151551662013\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 232 (single batch): 0.0004083785170223564\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 233 (single batch): 6.826368189649656e-05\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 234 (single batch): 0.0002726678503677249\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 235 (single batch): 0.00040121801430359483\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 236 (single batch): 0.0004118858487345278\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 237 (single batch): 0.0002742843935266137\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 238 (single batch): 0.0003239757788833231\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 239 (single batch): 0.0004282614681869745\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 240 (single batch): 0.00018507101049181074\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 241 (single batch): 0.00015030193026177585\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 242 (single batch): 0.00022863982303533703\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 243 (single batch): 0.00030158821027725935\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 244 (single batch): 0.0003715519269462675\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 245 (single batch): 0.00026012418675236404\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 246 (single batch): 0.0003677057393360883\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 247 (single batch): 0.00019395539129618555\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 248 (single batch): 0.0003016462142113596\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 249 (single batch): 0.00037101423367857933\n",
            "torch.Size([4, 512, 90])\n",
            "\t partial train loss 250 (single batch): 0.0003450540825724602\n",
            "beginning validation\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n",
            "torch.Size([4, 512, 90])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([176, 1, 512, 90])) that is different to the input size (torch.Size([176, 512, 90])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin plotting\n",
            "\n",
            " EPOCH 1/50 \t train loss 0.000 \t val loss 0.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-87062af01e7b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mplot_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-1436cc3d84ed>\u001b[0m in \u001b[0;36mplot_outputs\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdecoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdenoised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-67e47d5ce73c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x46080 and 184320x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for playing wav file\n",
        "sound, rate = librosa.load(\"/content/Honors-Thesis-Spencer-Ashford.git/trunk/misc/EndTrainingLoop.wav\")\n",
        "print('playing sound using  pydub')\n",
        "display(Audio(sound, rate=rate, autoplay=True))"
      ],
      "metadata": {
        "id": "zoDOb-yrAYBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}